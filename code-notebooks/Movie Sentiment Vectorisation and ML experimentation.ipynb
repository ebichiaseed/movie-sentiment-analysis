{"cells":[{"cell_type":"markdown","metadata":{"id":"34Dh6c7t_xcs"},"source":["# Summary\n","In this notebook, we explored two vectorization techniques—Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF)—using both versions of our preprocessed dataset from the previous data cleaning and exploratory data analysis phase. Version 1 of our dataset had regular stop words removed, while Version 2 had additional stop words removed. We evaluated four machine learning models: Logistic Regression, Naïve Bayes, Random Forest Classifier, and a suite of SVM-based methods, including a standard SVM, Linear SVC, and a Stochastic Gradient Descent Classifier.\n","\n","Our experiments revealed that the highest accuracy was achieved with the TF-IDF representation combined with regular stop word removal, particularly when using Logistic Regression and Naïve Bayes."]},{"cell_type":"markdown","source":["# Vectorisation and Experimentation of Machine Learning (ML) Techniques"],"metadata":{"id":"rRBoidBvWRGH"}},{"cell_type":"code","source":["!pip install torch_xla -f https://storage.googleapis.com/pytorch-tpu-releases/wheels/tpuvm/torch_xla.html\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uokVnKLvBXXK","executionInfo":{"status":"ok","timestamp":1743850740361,"user_tz":-480,"elapsed":17951,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"1a878f05-469c-41e0-ddc9-96d6025a0059"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://storage.googleapis.com/pytorch-tpu-releases/wheels/tpuvm/torch_xla.html\n","Collecting torch_xla\n","  Downloading torch_xla-2.6.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (21 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2025.1.31)\n","Downloading torch_xla-2.6.1-cp311-cp311-manylinux_2_28_x86_64.whl (93.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch_xla\n","Successfully installed torch_xla-2.6.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFp4sHSgNVu9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743850768634,"user_tz":-480,"elapsed":28266,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"fc3f2f96-bef9-4e29-af10-dbb34bd07a6c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n","WARNING:root:Found CUDA without GPU_NUM_DEVICES. Defaulting to PJRT_DEVICE=CUDA with GPU_NUM_DEVICES=1\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import SGDClassifier\n","\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","\n","import torch\n","import torch.nn as nn\n","\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","\n","import time"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ki9Nm9lLaHa2","executionInfo":{"status":"ok","timestamp":1743850775404,"user_tz":-480,"elapsed":21,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"5c8fcd1a-590e-47cf-ea55-6127a695d2fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"ES5PgbclCjAp"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"_ELBNGWM6VPt"},"source":["### Mounting Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73b3L4jz6X5O","executionInfo":{"status":"ok","timestamp":1743850799983,"user_tz":-480,"elapsed":22486,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"81dcab39-9ba3-4463-aa46-32b76c8d91b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8gD7ojFJ6jQC"},"source":["### Loading the Data"]},{"cell_type":"markdown","source":["Loading of cleaned data"],"metadata":{"id":"pMwr2TiF3QPK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAnTKzj6GKwK"},"outputs":[],"source":["pos_df_reg_stopwords = pd.read_csv('/content/drive/MyDrive/it1244/cleaned_pos_reviews_regular_stopwords.csv')\n","neg_df_reg_stopwords = pd.read_csv('/content/drive/MyDrive/it1244/cleaned_neg_reviews_regular_stopwords.csv')\n","\n","pos_df_extra_stopwords = pd.read_csv('/content/drive/MyDrive/it1244/cleaned_pos_reviews_extra_stopwords.csv')\n","neg_df_extra_stopwords = pd.read_csv('/content/drive/MyDrive/it1244/cleaned_neg_reviews_extra_stopwords.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvYOyNkaGMAT","executionInfo":{"status":"ok","timestamp":1743850807028,"user_tz":-480,"elapsed":5,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"00174c35-9cff-42c0-ba88-df8ba7a3a9e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["    FileName                                       Cleaned_Text\n","0  24556.txt  favorite part film wa old man attempt cure nei...\n","1  22787.txt  unlike comment mine positive movie wrap around...\n","2  24575.txt  different topic treated film straightforward s...\n","3  22772.txt  year old musical comedy fantasy might look age...\n","4  23617.txt  important film challenge viewer encourages pay...\n","    FileName                                       Cleaned_Text\n","0  23129.txt  not even goebbels could pulled propaganda stun...\n","1  22912.txt  plot fizzled reeked irreconcilable difference ...\n","2  23622.txt  first look cover picture look like good rock n...\n","3  23637.txt  drama core anna display genuine truth actor ag...\n","4  23109.txt  magic lassie opened radio city music hall wa f...\n","    FileName                                       Cleaned_Text\n","0  24556.txt  favorite part film wa old man attempt cure nei...\n","1  22787.txt  unlike comment mine positive movie wrap around...\n","2  24575.txt  different topic treated film straightforward s...\n","3  22772.txt  year old musical comedy fantasy might look age...\n","4  23617.txt  important film challenge viewer encourages pay...\n","    FileName                                       Cleaned_Text\n","0  23129.txt  even goebbels could pulled propaganda stunt li...\n","1  22912.txt  plot fizzled reeked irreconcilable difference ...\n","2  23622.txt  first look cover picture look like good rock n...\n","3  23637.txt  drama core anna display genuine truth actor ag...\n","4  23109.txt  magic lassie opened radio city music hall wa f...\n"]}],"source":["print(pos_df_reg_stopwords.head())\n","print(neg_df_reg_stopwords.head())\n","\n","print(pos_df_extra_stopwords.head())\n","print(neg_df_extra_stopwords.head())"]},{"cell_type":"markdown","metadata":{"id":"yCrJtaWS6tLw"},"source":["### Labelling Reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y00DZV51ddYV","executionInfo":{"status":"ok","timestamp":1743851129877,"user_tz":-480,"elapsed":14,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"785f37f8-6b44-441d-e7fe-be2be3c00934"},"outputs":[{"output_type":"stream","name":"stdout","text":["df_reg_stopwords:\n","    FileName                                       Cleaned_Text  label\n","0  24556.txt  favorite part film wa old man attempt cure nei...      1\n","1  22787.txt  unlike comment mine positive movie wrap around...      1\n","2  24575.txt  different topic treated film straightforward s...      1\n","3  22772.txt  year old musical comedy fantasy might look age...      1\n","4  23617.txt  important film challenge viewer encourages pay...      1\n","\n","df_extra_stopwords:\n","    FileName                                       Cleaned_Text  label\n","0  24556.txt  favorite part film wa old man attempt cure nei...      1\n","1  22787.txt  unlike comment mine positive movie wrap around...      1\n","2  24575.txt  different topic treated film straightforward s...      1\n","3  22772.txt  year old musical comedy fantasy might look age...      1\n","4  23617.txt  important film challenge viewer encourages pay...      1\n"]}],"source":["# This code labels positive and negative reviews - 1 for positive, 0 for negative, then combines them into two datasets for ease of access:\n","# one with regular stopwords and one with extra stopwords removed. It also prints a preview of both.\n","\n","# For the regular stopwords dataframes\n","pos_df_reg_stopwords['label'] = 1\n","neg_df_reg_stopwords['label'] = 0\n","\n","# Combine positive and negative into a single dataframe\n","df_reg_stopwords = pd.concat([pos_df_reg_stopwords, neg_df_reg_stopwords], axis=0, ignore_index=True)\n","\n","# For the extra stopwords dataframes\n","pos_df_extra_stopwords['label'] = 1\n","neg_df_extra_stopwords['label'] = 0\n","\n","# Combine positive and negative into a single dataframe\n","df_extra_stopwords = pd.concat([pos_df_extra_stopwords, neg_df_extra_stopwords], axis=0, ignore_index=True)\n","\n","print(\"df_reg_stopwords:\")\n","print(df_reg_stopwords.head())\n","\n","print(\"\\ndf_extra_stopwords:\")\n","print(df_extra_stopwords.head())\n"]},{"cell_type":"markdown","metadata":{"id":"wqL5PRzI7fky"},"source":["## Text Vectorisation"]},{"cell_type":"code","source":["# This code performs text vectorization on two datasets: one with regular stopwords and one with extra stopwords removed.\n","# It uses both Bag of Words (BoW) and TF-IDF approaches to convert the cleaned text data into numerical feature matrices.\n","# The resulting feature shapes are printed to compare the effect of stopword removal on vocabulary size and representation.\n","\n","\n","# Bag of Words (BoW) for Regular Stopwords\n","bow_vectorizer_reg = CountVectorizer(\n","    max_features=50000, # keep only the 50,000 most frequent tokens\n","    min_df=2, # ignore terms that appear in fewer than 2 documents\n","    lowercase=False, # assumes text is already lowercased\n","    tokenizer=lambda x: x.split(), # split the text on whitespace\n","    preprocessor=None, # disable default preprocessing pattern\n","    token_pattern=None # disable default token pattern\n",")\n","X_reg_bow = bow_vectorizer_reg.fit_transform(df_reg_stopwords['Cleaned_Text'])\n","y_reg = df_reg_stopwords['label']\n","print(\"Regular Stopwords - BoW shape:\", X_reg_bow.shape)\n","\n","# Bag of Words (BoW) for Extra Stopwords\n","# same as above, but with extra stopwords removed\n","bow_vectorizer_extra = CountVectorizer(\n","    max_features=50000,\n","    min_df=2,\n","    lowercase=False,\n","    tokenizer=lambda x: x.split(),\n","    preprocessor=None,\n","    token_pattern=None\n",")\n","X_extra_bow = bow_vectorizer_extra.fit_transform(df_extra_stopwords['Cleaned_Text'])\n","y_extra = df_extra_stopwords['label']\n","print(\"Extra Stopwords - BoW shape:\", X_extra_bow.shape)\n","\n","# TF-IDF for Regular Stopwords\n","\n","# the TF-IDF vectoriser has the same parameters as stated in BoW\n","tfidf_vectorizer_reg = TfidfVectorizer(\n","    max_features=50000,\n","    min_df=2,\n","    lowercase=False,\n","    tokenizer=lambda x: x.split(),\n","    preprocessor=None,\n","    token_pattern=None\n",")\n","X_reg_tfidf = tfidf_vectorizer_reg.fit_transform(df_reg_stopwords['Cleaned_Text'])\n","print(\"Regular Stopwords - TF-IDF shape:\", X_reg_tfidf.shape)\n","\n","# TF-IDF for Extra Stopwords\n","tfidf_vectorizer_extra = TfidfVectorizer(\n","    max_features=50000,\n","    min_df=2,\n","    lowercase=False,\n","    tokenizer=lambda x: x.split(),\n","    preprocessor=None,\n","    token_pattern=None\n",")\n","X_extra_tfidf = tfidf_vectorizer_extra.fit_transform(df_extra_stopwords['Cleaned_Text'])\n","print(\"Extra Stopwords - TF-IDF shape:\", X_extra_tfidf.shape)\n"],"metadata":{"id":"20j7gfIMesXs","executionInfo":{"status":"ok","timestamp":1743851147285,"user_tz":-480,"elapsed":14029,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0aefec3-2e1f-4fc6-8cf5-bae36c37811b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Regular Stopwords - BoW shape: (50000, 50000)\n","Extra Stopwords - BoW shape: (50000, 50000)\n","Regular Stopwords - TF-IDF shape: (50000, 50000)\n","Extra Stopwords - TF-IDF shape: (50000, 50000)\n"]}]},{"cell_type":"markdown","source":["## Function to Evaluate Model Performance"],"metadata":{"id":"SkVhEI1Vowd_"}},{"cell_type":"code","source":["# This function fits and evaluates a classification model using both a train-test split and 5-fold cross-validation.\n","# It prints accuracy, F1 score, recall, and precision on the test set, along with cross-validated accuracy statistics.\n","\n","def evaluate_model(model, X, y, name=\"\"):\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, stratify=y, random_state=42\n","    )\n","\n","    # Fit the model\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    # Basic metrics on hold-out test\n","    acc = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    rec = recall_score(y_test, y_pred)\n","    prec = precision_score(y_test, y_pred)\n","\n","    # 5-fold CV on entire dataset\n","    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy', n_jobs=-1)\n","\n","    print(f\"\\n=== {name} ===\")\n","    print(f\"Test Accuracy:    {acc:.4f}\")\n","    print(f\"Test F1 Score:    {f1:.4f}\")\n","    print(f\"Test Recall:      {rec:.4f}\")\n","    print(f\"Test Precision:   {prec:.4f}\")\n","    print(f\"5-Fold CV Accuracy: Mean = {cv_scores.mean():.4f}, Std = {cv_scores.std():.4f}\")"],"metadata":{"id":"noG4hAeyovWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_F8Qw9xFSjz"},"source":["# Model Experimentation"]},{"cell_type":"markdown","metadata":{"id":"sroYLkTroHv7"},"source":["## Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"FOMPiHljicX2"},"source":["In this section, we will:\n","- split data into 80 20 for Training and Testing respectively\n","- stratify data to ensure important subgroup differences are accounted for, improving the accuracy, fairness, and insightfulness of our analysis."]},{"cell_type":"markdown","source":["### BoW"],"metadata":{"id":"_cbqYc8m5RgU"}},{"cell_type":"code","source":["# This code trains and evaluates a Logistic Regression model using BoW features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","log_reg = LogisticRegression(solver='liblinear', max_iter=400, random_state=42)\n","\n","evaluate_model(log_reg, X_reg_bow, y_reg, name=\"Logistic Regression + BoW (Regular)\")\n","evaluate_model(log_reg, X_extra_bow, y_extra, name=\"Logistic Regression + BoW (Extra)\")\n"],"metadata":{"id":"wTLlJMyE6dy-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743736827639,"user_tz":-480,"elapsed":127111,"user":{"displayName":"Qingzhe Tan","userId":"10557092588754043899"}},"outputId":"7af287d1-6587-4ddb-fcc8-f16bed55f867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Logistic Regression + BoW (Regular) ===\n","Test Accuracy:    0.8810\n","Test F1 Score:    0.8820\n","Test Recall:      0.8898\n","Test Precision:   0.8744\n","5-Fold CV Accuracy: Mean = 0.8820, Std = 0.0035\n","\n","=== Logistic Regression + BoW (Extra) ===\n","Test Accuracy:    0.8803\n","Test F1 Score:    0.8813\n","Test Recall:      0.8890\n","Test Precision:   0.8738\n","5-Fold CV Accuracy: Mean = 0.8819, Std = 0.0032\n"]}]},{"cell_type":"markdown","metadata":{"id":"QnlNXyGm-_sS"},"source":["### TF-IDF"]},{"cell_type":"code","source":["# This code trains and evaluates a Logistic Regression model using TF-IDF features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","evaluate_model(log_reg, X_reg_tfidf, y_reg, name=\"Logistic Regression + TF-IDF (Regular)\")\n","evaluate_model(log_reg, X_extra_tfidf, y_extra, name=\"Logistic Regression + TF-IDF (Extra)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"podXplS1nfMJ","executionInfo":{"status":"ok","timestamp":1743736840486,"user_tz":-480,"elapsed":12848,"user":{"displayName":"Qingzhe Tan","userId":"10557092588754043899"}},"outputId":"b820c04c-8001-4e74-f1aa-8eb9a9fde4e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Logistic Regression + TF-IDF (Regular) ===\n","Test Accuracy:    0.8940\n","Test F1 Score:    0.8958\n","Test Recall:      0.9116\n","Test Precision:   0.8806\n","5-Fold CV Accuracy: Mean = 0.8926, Std = 0.0033\n","\n","=== Logistic Regression + TF-IDF (Extra) ===\n","Test Accuracy:    0.8932\n","Test F1 Score:    0.8950\n","Test Recall:      0.9102\n","Test Precision:   0.8803\n","5-Fold CV Accuracy: Mean = 0.8924, Std = 0.0032\n"]}]},{"cell_type":"markdown","metadata":{"id":"GTx656QUoKlC"},"source":["## Naive Bayes"]},{"cell_type":"markdown","source":["### BoW"],"metadata":{"id":"C7e3ZHEy4171"}},{"cell_type":"code","source":["# This code trains and evaluates a Naive Bayes model using BoW features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","nb_model = MultinomialNB()\n","\n","evaluate_model(nb_model, X_reg_bow, y_reg, name=\"Naive Bayes + BoW (Regular)\")\n","evaluate_model(nb_model, X_extra_bow, y_extra, name=\"Naive Bayes + BoW (Extra)\")"],"metadata":{"id":"ZZ8cmB0kcrEZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743227742764,"user_tz":-480,"elapsed":491,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"6728f862-cd05-4ca7-a411-1751e8595308"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Naive Bayes + BoW (Regular) ===\n","Test Accuracy:    0.8541\n","Test F1 Score:    0.8508\n","Test Recall:      0.8318\n","Test Precision:   0.8706\n","5-Fold CV Accuracy: Mean = 0.8548, Std = 0.0052\n","\n","=== Naive Bayes + BoW (Extra) ===\n","Test Accuracy:    0.8541\n","Test F1 Score:    0.8509\n","Test Recall:      0.8328\n","Test Precision:   0.8699\n","5-Fold CV Accuracy: Mean = 0.8549, Std = 0.0051\n"]}]},{"cell_type":"markdown","source":["### TF-IDF"],"metadata":{"id":"rD3GoDLK44ke"}},{"cell_type":"code","source":["# This code trains and evaluates a Naive Bayes model using TF-IDF features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","nb_model = MultinomialNB()\n","\n","evaluate_model(nb_model, X_reg_tfidf, y_reg,   name=\"Naive Bayes + TF-IDF (Regular)\")\n","evaluate_model(nb_model, X_extra_tfidf, y_extra, name=\"Naive Bayes + TF-IDF (Extra)\")"],"metadata":{"id":"vjQJvdaQ4819","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743227743308,"user_tz":-480,"elapsed":542,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"bb12c20a-cd48-467e-beae-34934b42ea34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Naive Bayes + TF-IDF (Regular) ===\n","Test Accuracy:    0.8640\n","Test F1 Score:    0.8622\n","Test Recall:      0.8512\n","Test Precision:   0.8736\n","5-Fold CV Accuracy: Mean = 0.8643, Std = 0.0037\n","\n","=== Naive Bayes + TF-IDF (Extra) ===\n","Test Accuracy:    0.8639\n","Test F1 Score:    0.8622\n","Test Recall:      0.8516\n","Test Precision:   0.8731\n","5-Fold CV Accuracy: Mean = 0.8645, Std = 0.0038\n"]}]},{"cell_type":"markdown","source":["## Random Forest Classifier"],"metadata":{"id":"s7U3JWlG4_-t"}},{"cell_type":"markdown","source":["### BoW"],"metadata":{"id":"JzoUY3Ri5BrB"}},{"cell_type":"code","source":["# This code trains and evaluates a Random Forest Classifier using BoW features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","\n","evaluate_model(rf_model, X_reg_bow, y_reg,   name=\"Random Forest + BoW (Regular)\")\n","evaluate_model(rf_model, X_extra_bow, y_extra, name=\"Random Forest + BoW (Extra)\")\n"],"metadata":{"id":"d5Y-D8XsbrWf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743228173177,"user_tz":-480,"elapsed":313641,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"3b8d0999-2bfa-481f-8281-4375536dc5aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Random Forest + BoW (Regular) ===\n","Test Accuracy:    0.8547\n","Test F1 Score:    0.8551\n","Test Recall:      0.8574\n","Test Precision:   0.8528\n","5-Fold CV Accuracy: Mean = 0.8523, Std = 0.0051\n","\n","=== Random Forest + BoW (Extra) ===\n","Test Accuracy:    0.8539\n","Test F1 Score:    0.8541\n","Test Recall:      0.8554\n","Test Precision:   0.8528\n","5-Fold CV Accuracy: Mean = 0.8534, Std = 0.0046\n"]}]},{"cell_type":"markdown","source":["### TF-IDF"],"metadata":{"id":"XaHV8zYT5D_d"}},{"cell_type":"code","source":["# This code trains and evaluates a Random Forest Classifier using TF-IDF features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","\n","evaluate_model(rf_model, X_reg_tfidf, y_reg,   name=\"Random Forest + TF-IDF (Regular)\")\n","evaluate_model(rf_model, X_extra_tfidf, y_extra, name=\"Random Forest + TF-IDF (Extra)\")"],"metadata":{"id":"aSfQ7xih5Fdz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743228453758,"user_tz":-480,"elapsed":280582,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"74e9f11a-bc2f-46bb-fbd9-1e73c3c9141f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Random Forest + TF-IDF (Regular) ===\n","Test Accuracy:    0.8497\n","Test F1 Score:    0.8496\n","Test Recall:      0.8492\n","Test Precision:   0.8501\n","5-Fold CV Accuracy: Mean = 0.8509, Std = 0.0054\n","\n","=== Random Forest + TF-IDF (Extra) ===\n","Test Accuracy:    0.8501\n","Test F1 Score:    0.8499\n","Test Recall:      0.8490\n","Test Precision:   0.8509\n","5-Fold CV Accuracy: Mean = 0.8515, Std = 0.0054\n"]}]},{"cell_type":"markdown","source":["## SVM"],"metadata":{"id":"q1rdqXBq4fL_"}},{"cell_type":"markdown","source":["#### Basic SVM Model"],"metadata":{"id":"Vs8dbKGI1deG"}},{"cell_type":"markdown","source":["Basic SVM Model Code\n","\n","Good for:\n","- Determining Probability Estimates\n","- Built-in multiclass OvO (One-vs-One (OvO) is a strategy for extending binary classifiers (like SVMs) to handle multi-class classification problems.)"],"metadata":{"id":"kMYvjJtmakf1"}},{"cell_type":"code","source":["# This code trains and evaluates a SVM with Linear Kernel using BoW features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","svm_basic = make_pipeline(\n","    StandardScaler(with_mean = False),\n","    SVC(kernel='linear', C=1.0, random_state=42)\n",")"],"metadata":{"id":"YALKc0uXU7CS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate_model(svm_basic, X_reg_bow, y_reg,   name=\"SVM + BoW (Regular)\")\n","evaluate_model(svm_basic, X_extra_bow, y_extra, name=\"SVM + BoW (Extra)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JB9Slto6VCKP","executionInfo":{"status":"ok","timestamp":1743270697184,"user_tz":-480,"elapsed":18365584,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"111b6b4b-20d5-43d9-e06e-ef23e2747124"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SVM + BoW (Regular) ===\n","Test Accuracy:    0.8356\n","Test F1 Score:    0.8358\n","Test Recall:      0.8370\n","Test Precision:   0.8347\n","5-Fold CV Accuracy: Mean = 0.8328, Std = 0.0043\n","\n","=== SVM + BoW (Extra) ===\n","Test Accuracy:    0.8337\n","Test F1 Score:    0.8337\n","Test Recall:      0.8340\n","Test Precision:   0.8335\n","5-Fold CV Accuracy: Mean = 0.8317, Std = 0.0050\n"]}]},{"cell_type":"code","source":["# This code trains and evaluates a SVM with Linear Kernel with TF-IDF features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","evaluate_model(svm_basic, X_reg_tfidf, y_reg,   name=\"SVM + TF-IDF (Regular)\")\n","evaluate_model(svm_basic, X_extra_tfidf, y_extra, name=\"SVM + TF-IDF (Extra)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3ZUo7KeVEeK","executionInfo":{"status":"ok","timestamp":1743297081527,"user_tz":-480,"elapsed":26384345,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"bf59191d-8a9e-4caa-c851-24a0355bcc39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SVM + TF-IDF (Regular) ===\n","Test Accuracy:    0.8276\n","Test F1 Score:    0.8285\n","Test Recall:      0.8326\n","Test Precision:   0.8244\n","5-Fold CV Accuracy: Mean = 0.8333, Std = 0.0035\n","\n","=== SVM + TF-IDF (Extra) ===\n","Test Accuracy:    0.8296\n","Test F1 Score:    0.8306\n","Test Recall:      0.8354\n","Test Precision:   0.8258\n","5-Fold CV Accuracy: Mean = 0.8324, Std = 0.0043\n"]}]},{"cell_type":"markdown","source":["#### LinearSVC Variant"],"metadata":{"id":"FqewCOgM1xzV"}},{"cell_type":"markdown","source":["LinearSVC Variant\n","\n","Good for:\n","- Analysing Large Dataset with BoW / TF-IDF\n","- Works well with high dimensions\n","\n","\n","We test it over a range of C to ensure that this model converges"],"metadata":{"id":"tXYj0u6dHw_j"}},{"cell_type":"code","source":["# This code trains and evaluates LinearSVC using BoW and TF-IDF features, evaluating the model based on different regularization strengths\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","# Each iteration is timed for comparison of time efficiency with SGDClassifier\n","\n","\n","# Loop over different regularization strengths\n","for c in [0.001, 0.01, 0.1]:\n","    print(f\"\\nEvaluating models with C={c}\")\n","\n","    # Model pipeline\n","    svc_model = make_pipeline(\n","        StandardScaler(with_mean=False),\n","        LinearSVC(C=c, max_iter=1_000_000, random_state=42)\n","    )\n","\n","    # Evaluate and time each dataset separately\n","    for X_data, y_data, name in [\n","        (X_reg_bow, y_reg, f\"SVM + BoW (Regular), C={c}\"),\n","        (X_extra_bow, y_extra, f\"SVM + BoW (Extra), C={c}\"),\n","        (X_reg_tfidf, y_reg, f\"SVM + TF-IDF (Regular), C={c}\"),\n","        (X_extra_tfidf, y_extra, f\"SVM + TF-IDF (Extra), C={c}\")\n","    ]:\n","        start_time = time.time()\n","        evaluate_model(svc_model, X_data, y_data, name=name)\n","        elapsed = time.time() - start_time\n","        print(f\"Time for {name}: {elapsed:.4f} seconds\")\n"],"metadata":{"id":"uVQmhxmcHcc4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743741299870,"user_tz":-480,"elapsed":3542667,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"344daf96-78cd-42ba-99a0-1bd7c7ddb7ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating models with C=0.001\n","\n","=== SVM + BoW (Regular), C=0.001 ===\n","Test Accuracy:    0.8478\n","Test F1 Score:    0.8483\n","Test Recall:      0.8512\n","Test Precision:   0.8455\n","5-Fold CV Accuracy: Mean = 0.8487, Std = 0.0037\n","Time for SVM + BoW (Regular), C=0.001: 72.7978 seconds\n","\n","=== SVM + BoW (Extra), C=0.001 ===\n","Test Accuracy:    0.8462\n","Test F1 Score:    0.8468\n","Test Recall:      0.8504\n","Test Precision:   0.8433\n","5-Fold CV Accuracy: Mean = 0.8468, Std = 0.0027\n","Time for SVM + BoW (Extra), C=0.001: 72.3999 seconds\n","\n","=== SVM + TF-IDF (Regular), C=0.001 ===\n","Test Accuracy:    0.8348\n","Test F1 Score:    0.8361\n","Test Recall:      0.8428\n","Test Precision:   0.8295\n","5-Fold CV Accuracy: Mean = 0.8358, Std = 0.0043\n","Time for SVM + TF-IDF (Regular), C=0.001: 61.7305 seconds\n","\n","=== SVM + TF-IDF (Extra), C=0.001 ===\n","Test Accuracy:    0.8310\n","Test F1 Score:    0.8322\n","Test Recall:      0.8382\n","Test Precision:   0.8263\n","5-Fold CV Accuracy: Mean = 0.8344, Std = 0.0027\n","Time for SVM + TF-IDF (Extra), C=0.001: 63.2731 seconds\n","\n","Evaluating models with C=0.01\n","\n","=== SVM + BoW (Regular), C=0.01 ===\n","Test Accuracy:    0.8358\n","Test F1 Score:    0.8361\n","Test Recall:      0.8378\n","Test Precision:   0.8345\n","5-Fold CV Accuracy: Mean = 0.8378, Std = 0.0040\n","Time for SVM + BoW (Regular), C=0.01: 295.6387 seconds\n","\n","=== SVM + BoW (Extra), C=0.01 ===\n","Test Accuracy:    0.8343\n","Test F1 Score:    0.8346\n","Test Recall:      0.8360\n","Test Precision:   0.8332\n","5-Fold CV Accuracy: Mean = 0.8361, Std = 0.0033\n","Time for SVM + BoW (Extra), C=0.01: 262.9943 seconds\n","\n","=== SVM + TF-IDF (Regular), C=0.01 ===\n","Test Accuracy:    0.8305\n","Test F1 Score:    0.8316\n","Test Recall:      0.8368\n","Test Precision:   0.8264\n","5-Fold CV Accuracy: Mean = 0.8330, Std = 0.0042\n","Time for SVM + TF-IDF (Regular), C=0.01: 254.9977 seconds\n","\n","=== SVM + TF-IDF (Extra), C=0.01 ===\n","Test Accuracy:    0.8264\n","Test F1 Score:    0.8274\n","Test Recall:      0.8320\n","Test Precision:   0.8228\n","5-Fold CV Accuracy: Mean = 0.8315, Std = 0.0027\n","Time for SVM + TF-IDF (Extra), C=0.01: 262.5935 seconds\n","\n","Evaluating models with C=0.1\n","\n","=== SVM + BoW (Regular), C=0.1 ===\n","Test Accuracy:    0.8351\n","Test F1 Score:    0.8353\n","Test Recall:      0.8362\n","Test Precision:   0.8344\n","5-Fold CV Accuracy: Mean = 0.8338, Std = 0.0043\n","Time for SVM + BoW (Regular), C=0.1: 557.9153 seconds\n","\n","=== SVM + BoW (Extra), C=0.1 ===\n","Test Accuracy:    0.8319\n","Test F1 Score:    0.8321\n","Test Recall:      0.8332\n","Test Precision:   0.8310\n","5-Fold CV Accuracy: Mean = 0.8318, Std = 0.0032\n","Time for SVM + BoW (Extra), C=0.1: 521.0547 seconds\n","\n","=== SVM + TF-IDF (Regular), C=0.1 ===\n","Test Accuracy:    0.8300\n","Test F1 Score:    0.8310\n","Test Recall:      0.8358\n","Test Precision:   0.8262\n","5-Fold CV Accuracy: Mean = 0.8328, Std = 0.0045\n","Time for SVM + TF-IDF (Regular), C=0.1: 574.7661 seconds\n","\n","=== SVM + TF-IDF (Extra), C=0.1 ===\n","Test Accuracy:    0.8259\n","Test F1 Score:    0.8268\n","Test Recall:      0.8312\n","Test Precision:   0.8225\n","5-Fold CV Accuracy: Mean = 0.8310, Std = 0.0029\n","Time for SVM + TF-IDF (Extra), C=0.1: 542.5815 seconds\n"]}]},{"cell_type":"markdown","source":["#### Stochastic Gradient Descent (SGD) Classifier"],"metadata":{"id":"geUF5BbH1rgj"}},{"cell_type":"markdown","source":["SGDClassifier\n","\n","To combat convergence failures by approximating Linear SVMs with stochastic gradient descent and is more stable for large/sparse data\n","\n","- Very fast on large datasets (text, image vectors, etc.)\n","- Works well with sparse data (e.g., TF-IDF)\n","- Easy to tune with GridSearchCV"],"metadata":{"id":"mPh3JnU_UffO"}},{"cell_type":"code","source":["# This code trains and evaluates a SGDClassifier using both BoW and TF-IDF features.\n","# It compares performance on two datasets: one with regular stopwords and one with extra stopwords removed.\n","\n","\n","# SGD model pipeline\n","sgd_model = make_pipeline(\n","    StandardScaler(with_mean=False),\n","    SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",")\n","\n","# Evaluate on BoW (Regular)\n","start = time.time()\n","evaluate_model(sgd_model, X_reg_bow, y_reg, name=\"SGD SVM + BoW (Regular)\")\n","print(f\"Time taken for SGD SVM + BoW (Regular): {time.time() - start:.4f} seconds\\n\")\n","\n","# Evaluate on BoW (Extra)\n","start = time.time()\n","evaluate_model(sgd_model, X_extra_bow, y_extra, name=\"SGD SVM + BoW (Extra)\")\n","print(f\"Time taken for SGD SVM + BoW (Extra): {time.time() - start:.4f} seconds\\n\")\n","\n","# Evaluate on TF-IDF (Regular)\n","start = time.time()\n","evaluate_model(sgd_model, X_reg_tfidf, y_reg, name=\"SGD SVM + TF-IDF (Regular)\")\n","print(f\"Time taken for SGD SVM + TF-IDF (Regular): {time.time() - start:.4f} seconds\\n\")\n","\n","# Evaluate on TF-IDF (Extra)\n","start = time.time()\n","evaluate_model(sgd_model, X_extra_tfidf, y_extra, name=\"SGD SVM + TF-IDF (Extra)\")\n","print(f\"Time taken for SGD SVM + TF-IDF (Extra): {time.time() - start:.4f} seconds\\n\")\n"],"metadata":{"id":"UvAKjhczUTHW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743851169303,"user_tz":-480,"elapsed":20462,"user":{"displayName":"erika c.","userId":"04643775268813433208"}},"outputId":"ca9c46c8-ff1c-4742-b91d-2bd7a9b9981e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SGD SVM + BoW (Regular) ===\n","Test Accuracy:    0.8238\n","Test F1 Score:    0.8243\n","Test Recall:      0.8264\n","Test Precision:   0.8221\n","5-Fold CV Accuracy: Mean = 0.8257, Std = 0.0049\n","Time taken for SGD SVM + BoW (Regular): 10.3035 seconds\n","\n","\n","=== SGD SVM + BoW (Extra) ===\n","Test Accuracy:    0.8208\n","Test F1 Score:    0.8211\n","Test Recall:      0.8224\n","Test Precision:   0.8198\n","5-Fold CV Accuracy: Mean = 0.8232, Std = 0.0047\n","Time taken for SGD SVM + BoW (Extra): 4.3011 seconds\n","\n","\n","=== SGD SVM + TF-IDF (Regular) ===\n","Test Accuracy:    0.8107\n","Test F1 Score:    0.8119\n","Test Recall:      0.8170\n","Test Precision:   0.8068\n","5-Fold CV Accuracy: Mean = 0.8122, Std = 0.0044\n","Time taken for SGD SVM + TF-IDF (Regular): 3.5189 seconds\n","\n","\n","=== SGD SVM + TF-IDF (Extra) ===\n","Test Accuracy:    0.8072\n","Test F1 Score:    0.8087\n","Test Recall:      0.8152\n","Test Precision:   0.8024\n","5-Fold CV Accuracy: Mean = 0.8111, Std = 0.0044\n","Time taken for SGD SVM + TF-IDF (Extra): 2.3023 seconds\n","\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1hmXN_FqaYygGc1cAOqsPZj84c991HEwg","timestamp":1743320393835},{"file_id":"1rrpMR2ST4u8RraoCrCsSP1t0R19XnV1-","timestamp":1743175477024},{"file_id":"1m9KgfmXY3X2dHr9z0VLAn3CcRzJioJLa","timestamp":1742916044208}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}